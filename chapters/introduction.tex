\label{introduction}
The Internet is composed of the clear web which contains indexed content and is accessible via standard browsers. However, it is also composed of the dark web. Content in the dark web is not indexed and accessible through special browsers. The priority of the dark web is anonymity.

Anonymity and security is meaningful to whistle-blowers as their actions may have severe consequences otherwise. Human-rights activists also risk persecution in oppressed parts of the world and therefore benefit from keeping their identity secret. However, with anonymity comes a lack of accountability. But accountability is important for the prevention of illegal activities. The dark web is thus also an environment with illegal content and services, e.g. child porn, fake identities, or money laundering. The monitoring of the dark web is helpful in shutting down such services.

An industrial partner of the Masaryk University was interested in the structure of the dark net. More specifically, information about page categories, connections, and contents was sought. An application able to supply the partner with the before mentioned information needed to be developed. Additionally, the partner expected the application to run on UNIX systems.

This thesis was aimed to provide the partner with the above mentioned application. A data set was acquired by Juraj Noge as part of his bachelor's thesis \cite{bcScraping}. The data set comprised pages scraped from the dark web and was stored in Elasticsearch. These pages were made available to us and would constitute the data used in our application.

In this work we describe the dark web and analyse the provided data set in Chapter \ref{datasetAnalysis}. The quantity of the pages represented several obstacles. The difficulty related to the classification is explained in Chapter \ref{classification}. Machine learning and the approach chosen for the categorization of pages is also discussed in that chapter. Another complication, related to the display of the data is mentioned in Chapter \ref{clustering}.In addition, we outline what a web graph is and introduce two algorithms for community detection, the Louvain and the Leiden algorithm. Chapter \ref{developmentIntroduction} details how the gathered knowledge in the previous chapters is applied in the implementation of the application. We compare the output of the two community detection algorithms in Chapter \ref{evaluation}. This chapter also compared the accuracy achieved by the classification model trained on different learning data sets. The last Chapter \ref{conclusion} states the result of this work and suggests future improvements.

