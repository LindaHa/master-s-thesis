\label{developmentIntroduction}
An application was developed and a CNN was trained in order to fulfill the goals of this thesis. One task of the application was to classify the scraped pages into categories. This was to be done depending on the content of the pages. The application performs the classification via the model created by the CNN. CNNs and ANNs are described in section \ref{artificialNeuralNetworks}. Another task was to provide a way to observe the structure of the pages, links between them, and their categories. This was achieved by detecting communities of a graph. Communities and how to detect them are described in chapter \ref{clustering}. One of the partner requirements was for the application to function on a UNIX system. Another requirement was a user friendly UI. Also, the option for retrieval all the available information about the pages was demanded. 

\begin{figure}
  \centering
  \includegraphics[width = \textwidth]{Images/ApplicationArchitecture.png}
  \caption{The visualization of the architecture of the web application.}
  \label{ApplicationArchitecture}
\end{figure}

The architecture of the application is portrayed in figure \ref{ApplicationArchitecture}. This chapter first describes the implementation of the model for the classification of the pages. Afterwards the implementation of the clustering is introduced. Then the design and implementation of the application which is composed of a representational state transfer (REST) application program interface (API) and a front-end (FE) web application is detailed. 

\section{Classification} \label{ClassificationDevelopment}
To categorize the pages depending on their content a CNN needed to be created. For that a project called \texttt{Categorization} was built. For the training of a CNN a suitable training and testing data set needed to be prepared. The desired output of the CNN was a model able to categorize the pages with a reasonable accuracy. This chapter describes the steps taken in order to achieve the listed goals along with the technologies used.

\subsection{Technology overview} \label{ClassificationTechonologyOverview}
The project was written in Python. \textit{Python} is a widely used interpreted programming language known for readability and portability \cite{aboutPython}. It is open-source and is considered to have an extensive documentation and community available. Python is popular in the science community because it is easy to learn and has simple syntax. There is therefore a considerable amount of useful libraries for research purposes such as \textit{Keras} described later in this subsection, or \textit{igraph} introduced in subsection \ref{ClusteringTechonologyOverview}.  

The learning approach used in this thesis was SLr introduced in subsection \ref{supervisedLearning}. A sample data set with assigned categories was therefore needed. To view and edit a data set with thousands of rows the software EmEditor \cite{emeditor} was leveraged. EmEditor is a paid text editor with a free trial period. The advantage of this editor is its support of large files. Supported file formats include comma-separated values (CSV) files. 

For the modification of the content of the pages we utilized the library Natural Language Toolkit (NLTK) \cite{nltk}. NLTK is a Python library used for operating on human language data. NLTK provides functionality such as tokenization, or stemming. The library supports several languages including English. This library comes with detailed documentation. The classification of this thesis supports only English.

For the management of large matrices and vectors we used the library NumPy \cite{numpy}. NumPy is a Python library used for scientific computing. The functionality includes for example the formation of n-dimensional array objects, the random shuffling of rows of such an object, and random number capabilities. 

To manipulate the data set efficiently the pandas library \cite{pandas} was adopted. Pandas is an open-source library used for the manipulation of data frame\footnote{A data frame is a two-dimensional structure resembling a common table. Data stored in columns contains values designated for the same purpose. } objects. It is possible to slice through, add data to, or remove data from the data frames. Also, data frames may be merged or reshaped. Supported file formats include CSV and excel spreadsheet (XLS) files.

The CNN itself was created with the library Keras \cite{keras}. Keras is a library for the implementation of ML in Python. ML is more closely characterized in the section \ref{machineLearning}. Keras supports the tokenization of text and the tools needed to build and train a CNN. The output model of the CNN can be used for the task it was trained for.

\subsection{Learning data set} \label{LearningDatasetImplementation}
A labeled data set for learning was needed. We therefore built a learning data set composed of a subset of the scraped pages. The pages were retrieved from the database with equivalent fetching functionality as is described in subsection \ref{APIImplementation}. This functionality is located in the folder \texttt{Helpers}. Only one page with content of each domain from the Tor network was stored. 4,088 pages were obtained this way. These pages were with content exported into a CSV file and labeled manually. 13 categories were identified. We will call this learning data set \textit{first samples} in a later chapter. The problem of this data set was some categories contained less than 100 pages. The training result for those categories were not satisfying. Detailed training results performed on various training data sets are shown in section \ref{classificationEvaluation}. 

By the merging of categories with less than 100 pages into bigger categories we achieved better training results. One category, \textit{Gambling}, could not be merged with other categories as it was not related enough to any other category. We therefore enhanced Gambling with more pages from the database. This was possible because we found a domain with more than 200 pages with different content all associated to gambling. This data set contained 4.298 pages and 10 categories. We will further call this learning set \textit{enhanced samples}. However, a moral issue related to the naming conventions of the categories arose with this data set. 

The final data set was created by further merging and renaming the categories. 9 categories were identified. We will call this data set \textit{final samples}. The 9 categories detected are detailed in listing \ref{classificationCategories}.
 \label{classificationCategories}
\begin {description}
	\item[Finance and Fraud] contained 665 pages. It included content about fake or stolen credit, debit and gift cards and bank accounts. Also crypto currency, counterfeit money, and money laundering was mentioned. Suspicious and illegal investment opportunities were offered as well.
	\item[Gambling] contained 231 pages. Content involving casinos, bets and other form of gambling constituted this category.
	\item[Hosting and Programming and Hacking] contained 411 pages. Content assigned with this category involved technical blogs and hosting servers. Hacking tutorials and services were also offered.
	\item[Illegal services and goods] contained 139 pages. Provided services involved the hiring of hitmen\footnote{A hitman is person hired to kill another person or people.} and fake identity services. Offered goods included drugs and guns.
	\item[Online Marketplace] contained 143 pages. Content labeled with this category consisted of mentioning products of other categories along with electronics, clothing and accessories. All of these goods were offered at once on one page.
	\item[Other] contained 806 pages. Pages in this category were not possible to categorize. The content was either not English, too incoherent, short, or vague. 
	\item[Sexual Content] contained 1196 pages. Content in this category was of a sexual nature including sexual stories and the mentioning of images or videos.
	\item[Social] contained 136 pages. This content included blogs, chat rooms and information channels. Also censored content was found, such as the Bible or confidential information. Other content found was written works of art and mentions about paintings and music. 
\end{description}

\subsection{Implementation} \label{ClassificationImplementation}
The preprocessing of the learning data and the training of the CNN takes place in the file \texttt{KerasModelDataset.ipynb}. 

The learning data are loaded as a data frame. The data preprocessing is performed on the content of the pages. Non-English rows, also called documents, are removed. This is done by first removing non-numeric and non-alphabetical characters. Then the content is tokenized\footnote{A tool which divides a string into sub-strings.}. In this case the tokens represent words divided by spaces. The tokens are then compared to the English vocabulary of NLTK. According to the percentage of the English words composing the content the document is considered English or non-English. The threshold of English documents is 20\%. Non-English documents are removed from the data frame.

Next the labels need to be preprocessed. The labels are retrieved from the data frame by removing duplicates from the category column. Then a dictionary\footnote{A python dictionary is a hash table with keys and values.} with the category name as key and the index\footnote{An index in context of this chapter is an integer value corresponding to the position of an entry. It starts from 0.} as value is created. The indexes are used as category aliases. 

Afterwards two empty lists are initialized, \texttt{texts} and \texttt{labels}. For each row of the data frame the content is appended to list \texttt{texts} and the category is converted to the corresponding alias and appended to the list \texttt{labels}. The text is associated with the corresponding label via the indexes in the lists.  This is explained on an example. The row on position $i$ contains content $T_i$ and the corresponding category $C_i$. Content $T_i$ is appended to \texttt{texts} and is on the $i$th position. The alias for category $C_i$ is $C^a_i$. Now $C^a_i$ is appended to \texttt{labels} and is also on the $i$th position.

Next an internal vocabulary is initialized from all words from \texttt{texts}. Each word $w$ of each entry in \texttt{texts} is represented by an integer $n_w$. Each entry of \texttt{texts} therefore corresponds to a vector of integers. All vectors are trimmed or padded to the same length of 1,000 integers. The list \texttt{texts} corresponds to a matrix. The matrix is stored in a list of matrices called \texttt{data}. The list \texttt{data} is split into a training and testing set. One fifth of \texttt{data}, meaning 864 entries, is randomly chosen to be the test set. The rest is the training data.

Each category alias in \texttt{labels} is assigned an index $i$. Each entry of the list is replaced by a one hot vector (OHV)\footnote{A zero vector with one value being 1.} . The value 1 is on the $i$th position of the OHV. The length of the OHV corresponds to the number of categories. The list \texttt{labels} becomes a matrix after this modification, meaning a list of vectors.

The next step is the configuration of embeddings. Embeddings are outlined in subsection \ref{embeddingLayers}. An open source embedding matrix is available on the Stanford university web site \cite{embeddings}. However, using that matrix changed the learning results only marginally as opposed to training the embedding layer on our data. One of the reasons might be the vocabulary and the content syntax of the dark web being different from the vocabulary used in the trained embeddings file. We therefore decided not to use the trained embeddings.

The last step before the training itself is to create the shape of the CNN. The Keras documentation offers a sample of a CNN structure meant for text classification \cite{kerasCNNStructure}. We used this structure and figure \ref{structureOfCNN} depicts the structure composition. 

The EmbLr receives information about the vocabulary size, the number of words on each page and the length of the output vectors. The vocabulary is limited to 20,000 most frequently occurring words. The number of words in the content of each page is limited to 1,000. The EmbLr therefore expects an input of vectors with the length 1,000. The output of this layer are embedding vectors with the length of 100. 

The second layer is a 1D ConvLr with 128 neurons and a kernel with the dimensions 5x1. The activation function of this layer is ReLU described in subsection \ref{activationFunction}. Next a PoolLr with filter dimensions of 5x1 is prepared. The method used in the pooling layer is max-pooling. The next layer is a 1D ConvLr followed by a PoolLr and another 1D ConvLr all equivalent to the first layers of the corresponding types. The seventh layer is a PoolLr also using the max-pooling method but with the filter dimensions equal to the input dimensions. Then a dense layer with 128 neurons and ReLU as the activation function follows. The last layer is a dense layer with the activation function softmax. The number of neurons in this layer is the number of categories.

Afterwards a Keras model is fed the CNN structure and compiled. The configured loss function of the model is CCross. The desired metrics to be evaluated is the accuracy of the results. The optimizer used  is RMSprop. The model is then trained with training and testing data. The number of epochs is set to 20 and batch\_size is 128.

After the training finished the model was exported. It then was used in the BE for the categorization of the pages.

\section{Clustering}\label{ClusteringDevelopment}
The structure of the pages was to be visualized in the form of a graph. This amount of pages needed to be divided into groups in order to view the structure without the loss of information. For this purpose both LA and LeA were used separately. These algorithms are characterized in sections \ref{louvainAlgorithm} and \ref{leidenAlgorithm} of chapter respectively. 

\subsection{Technology overview} \label{ClusteringTechonologyOverview}
In the beginning we used the library \textit{NetworkX} \cite{networkX} for the graph creation. NetworkX is a Python library for the managing of complex networks. The advantages of this library are convenient documentation. Also, the graphs of NetworkX cover many characteristics such as undirected and directed, or weighted edges. It is possible to filter isolates from the graph.  Another advantage is the availability of standard graph algorithms, including LA. However, the built LA implementation was too slow for the size of our data set. The built in LA didn't succeed to divide about 40,000 pages after 30 minutes. We therefore decided to use a different implementation. The \textit{cylouvain} library \textit{cylouvain} was introduced. 

Cylouvain is a faster implementation of LA. The division of 90,275 pages into communities took about 18 seconds. The problem we faced with this implementation was the occasional occurrence of disconnected sub-communities of communities. Our new priority was therefore to adopt an implementation of LeA. The \textit{leidenalg} \cite{leidenalg} library was introduced.

Leidenalg is an \textit{C++} implementation of the LeA exposed to python. This implementation takes about 6 seconds to partition the 90,275 pages into communities. Moreover, there are no disconnected sub-communities of communities. Leidenalg is built on the \textit{python-igraph} library. 

Python-igraph (igraph) another library for the managing of complex networks. The advantage of this library the availability of the before mentioned implementation of the LeA. Another advantage is igraph disposes of an implementation of the LA. This implementation takes about 5 seconds to divide 90,275 pages into communities. Also, no disconnected sub-communities of communities are created. Therefore this implementation was also leveraged.

The two utilized implementations are compared in the section \ref{clusteringEvaluation}.

\subsection{Implementation} \label{ClusteringImplementation}
The clustering is implemented in the back-end application. The application is further described in the next section \ref{APIDevelopment}. We describe the clustering in this subsection. 

The clustering itself (cluster cycle) is executed in the method \\ \texttt{get\_groups\_without\_links\_and\_isolates} in the \texttt{graph\_helpers.py} file. The entry parameter are either scraped pages or communities. Both will act as nodes. The implementations of the clustering algorithms described in the previous subsection \ref{ClusteringTechonologyOverview} require the nodes to be in igraph format. To build an igraph from nodes we first need to convert the nodes into integer aliases. The aliases will constitute the vertices of the graph. The node links also need to be in the form of integer aliases. These link aliases will compose the edges of the graph. Now the graph which was built using the igraph library can be filled with the edges and the vertices. Next, isolates are filtered out and deleted from the graph vertices. Afterwards it is decided whether to use the LA or the LeA implementation. The decision is made based on a boolean flag. Both implementations return a partition. This partition is a dictionary with the node aliases as keys and the communities assigned to the nodes as values. At last the aliases in the dictionary are converted back to the original node ids\footnote{Ids in case of pages are urls. Ids in case of communities are specific strings}. The method returns the partitioned node ids and the isolates if any were detected. 

The clustering cycle is repeated on communities until the number of detected communities is not greater than the desired maximum number (max count). This is demonstrated in figure \ref{clusteringCyclesExample}. Another reason to end the repetition is if the number of detected communities cannot be reduced any further. Max count in this thesis is 50.
\begin{figure}[ht!]
  \centering
  \includegraphics[width=\textwidth]{Images/clusteringCyclesExample.png}
  \caption{An example of multiple cluster cycles. Max count is four. The image on the left represents a hypothetical output of the first cluster cycle. It contains 9 squares each of a different colour with undirectional links between them. Each square depicts a community. The number of the communities is bigger than max\_count. Therefore the cluster cycle is repeated with the 9 communities as the nodes to be clustered. The second cluster cycle detects three communities portrayed as rectangles in the right image. This number satisfies the max count condition and the cluster cycle is not repeated.}
  \label{clusteringCyclesExample}
\end{figure} 

\section{API}\label{APIDevelopment}
The scraped pages of the dark-web were being stored in \textit{ElasticSearch}. We created a back-end application (BE) in order to perform various operations on the data-set before sending it to the FE. Such operations involve resource intensive processing of large volumes of data, and caching. We decided to initialize a Python BE. The reason behind this decision was the requirement for the application to function on a UNIX system. Other reasons are described in subsection~\ref{ClassificationTechonologyOverview}. The requirements for the BE was the categorization of the nodes along with their division into groups. The groups were either represented by nodes of the same category or communities. The BE also needed functionality to return details for specific pages or groups if requested.
\subsection{Technology overview}
\label{technologyOverview}
The BE was written in Python. As we wanted to follow the REST architecture we decided to make use of the \textit{Django} framework \cite{meetDjango}. Django is responsible for tasks such as running the server or managing web requests. Another advantage of Django is its \textit{Django REST framework} (DRF). DRF offers a convenient way for creating restful endpoints and responses \cite{djangoRest}. Both frameworks are open-source with helpful documentation and community. 

We had to solve performance issues. It took approximately 30 minutes to retrieve and categorize about 220, 314 pages from the database and circa 13 seconds to divide such a response into communities. We considered this wait time to be too long. Therefore we decided to cache the data of the first response. For that purpose \textit{Redis} \cite{redis} was used. It is an open-source solution which we used as a key-value store. It supports basic data structures\footnote{Simple structures, e.g. strings, numbers or sets.} as values, but not custom objects. Since the API uses custom objects for \textit{communities}, \textit{pages} and \textit{links}, an object serializer was leveraged along with Redis. We decided not to write our own but to utilise the Python \textit{pickle} module\footnote{A module used for converting Python objects to streams of bytes and vice versa.} \cite{pickle} (pickle). The reason behind this decision was the simplicity of pickle. Pickle also fulfilled all our needs for serializing. Specifically the serialization or deserialization of data in the form of the previously mentioned models for Redis to store in an acceptable time. Pickle took about 6 seconds to serialize 220, 314 pages. The deserialization of the same number of pages took about 7 seconds. However, the combination of pickle and Redis was not stable enough for the storing of sizable objects. The acquisition of page content was one of the tasks needed of this thesis. As page content was a sizable object we decided to utilize the Python module \textit{shelve} \cite{shelve} (shelve). Shelve manages \textit{shelfs}. A shelf file is a persistent python dictionary. The advantages of using shelve are the possibility of storing large objects and no need for serialization of keys or values. Shelve took approximately 61 seconds to retrieve 220, 314 pages with content. 

\subsection{Implementation} \label{APIImplementation}
The BE is implemented in the project \texttt{Dark-web-categorization-BE}. The projects was initialized in the \texttt{category} folder. The \texttt{settings.py} and \texttt{wsgi.py} files were included in the Django boilerplate. File \texttt{urls.py} contains the endpoints for the whole application. The urls relevant to this thesis are from the \texttt{api} module and imported into this file.
The \texttt{api} module is situated in the \texttt{api} folder. In the file \texttt{urls.py} routes with corresponding \textit{ViewSets}\footnote{A ViewSet is a class which provides responses for the corresponding endpoints. The request methods \textit{GET} and \textit{POST} are supported.} are registered to a Django \textit{router}\footnote{An object which assigns ViewSets to the corresponding endpoints.}. The file \texttt{apps.py} contains the boilerplate configuration file for the API. 

The \texttt{classification} folder contains the categorization functionality. The file \texttt{labels.py} lists the available categories. In \texttt{categorizer.py} the class \texttt{Categorizer} is created. The class first loads the tokenizer and the trained model. Then the model is compiled with the same attributes as described in \ref{ClassificationImplementation}. Both the tokenizer and the model are utilized in the method \texttt{categorize}. This method expects the page content as parameter. It applied the tokenizer to the received content. The output is padded or concatenated to the same length of 1,000 digits. The model predicts the category alias of the modified content. Lastly, the alias is converted to the name of the category.

There are several models situated in the \texttt{models.py} file. They are depicted in figure \ref{BEmodelsDiagram} along with the relationships between them. Each BE object used in the client-server communication needs to be transformed into a client-supported format before sending the object to the client. For this purpose serializers\footnote{A serializer transforms Django models into different formats, e.g. JSON.} are adopted. They are located in the file \texttt{serializers.py}.

File \texttt{view\_sets} stores the ViewSets of the API. This API handles 4 use-cases. One of which handles the acquisition of pages divided into communities depending solely on links between the pages. The next use-case manages the community detections based on categories. This means on the highest the pages are divided into groups solely by page category. On lower levels the pages are divided into communities depending on the links between the pages. The third use-case is the retrieving of further details of a specified page. The last use-case is the retrieval of group details\footnote{page details for all pages of a specified group.}. The ViewSet assigned to a called endpoint receives a request with optional parameters in the urls address or in the body of the request. Depending on the parameters the result is computed. 

We now detail a common procedure for a request. Let us assume the API received the request 
\texttt{GET /api/pages/bylink/?content-type=json\&id=2.0}. The client now expects the API to return sub-communities of the community with the id \textit{2.0}. The ViewSet \texttt{GroupsByLinkViewSet} is assigned to the used endpoint. The method used in the request is \textit{GET}. Therefore the \textit{list} function of \texttt{GroupsByLinkViewSet} is called. The url provided contains the \texttt{id} parameter. Now the method helper method \texttt{get\_subgroups\_of\_group} is called with the provided \texttt{id} is called. This method is located in the \texttt{level\_helpers} file situated in the \texttt{utils/graph\_helpers} folder. The method caches and returns a list of \texttt{Group} objects. However, this response would be too sizable for the server-client communitcation. That is why the \texttt{Group} objects are converted to \texttt{Meta\_Group} objects. A \texttt{Meta\_Group} object doe not contain all of the community members. It contains only the first 10 members as \texttt{Page} objects. \texttt{Meta\_Group} also contains an attribute representing the number of the members. The list of \texttt{Meta\_Group} objects is serialized via the \texttt{MetaGroupSerializer}. At last a \texttt{Response} object is constructed with the serialized result as its data. The response is returned to the client.

The folders \texttt{utils} and \texttt{utils/graph\_helpers} contain various helper methods. These methods are leveraged for example for the partition of the graph, caching or page detail gathering.

The file \texttt{repositories.py} holds the methods which handle the fetching from the database. Method \texttt{basic\_search} is used for the filtering of nodes based on page-content or page-ulr. Method \texttt{fetch\_chunk} retrieves portions with pages called \textit{chunks} from the database. In this thesis a chunk contains 500 pages. Each response from the database holds a chunk and a \textit{scroll\_id}. This id is sent in the next request to the database. The enclosed chunk in the next response from Elasticsearch  depends on the scroll\_id. The search context of Elasticsearch is kept open for 1 minute. This is defined by \texttt{scroll} attribute. Each request extends this time by another minute. The method \texttt{fetch\_all} gets all pages from the database. In this method \texttt{fetch\_chunk} is called until the response carries no pages. Each chunk is converted to a dictionary of \texttt{Page} objects with the page url as key and the whole page as value. Also, each batch is stored on the disc in a \textit{shelf}. The keys in the shelf are indexes of the batches and the values are the whole batches. After all pages from the database are obtained the content is removed from the pages. Also, invalid links are deleted from the pages. Then the pages are cached in Redis.
\begin{figure}[ht!]
  \centering
  \includegraphics[height=0.9\textheight]{Images/BEmodelsDiagram.png}
  \caption{A class diagram of the classes used in the BE. The diagram was created leveraging Visual Paradigm \cite{visualParadigm}}
  \label{BEmodelsDiagram}
\end{figure} 
 


\section{Front-end}
For users to be able to see the data acquired from the BE in a reasonable way a FE application was created. The goal of this FE was to visualize the scraped pages in a graph. The pages or communities, and links from the BE were to depict nodes, and links of the graph respectively. The category of the pages or communities was to be readable from the graph. Additional information about the nodes needed to be displayed or retrieved on demand.

\subsection{User interface}
We designed the UI the following way. 

After the application is loaded the UI is composed of a \textit{header} with the name of the application \textit{Dark web categorization}, a \textit{loader} and a \textit{sidebar} on the right hand side. The application resembles at this point the image in Figure \ref{zeroLevelGraphBasic}.
\begin{figure}[ht!]
  \centering
  \includegraphics[width=\textwidth]{Images/basic_view.png}
  \caption{The view after the application has loaded.}
  \label{zeroLevelGraphBasic}
\end{figure} 

The \textit{sidebar} contains several \textit{input fields} and \textit{buttons} in a column. At the very top of the \textit{sidebar} a \textit{drop-down button} (DD) is present. 

The DD sets the mode according to which the pages are grouped into communities. There are two modes available, \textit{link-mode} and \textit{category-mode} as can be seen in Figure\ref{modeDropdown}. If \textit{link-mode} is selected, the pages are divided by the LA\ref{louvainAlgorithm} depending on the connections between them only. If \textit{category-mode} is selected, the pages are divided into groups by categories. The pages in the groups are further divided into communities according to links between them, as if in \textit{link-mode}. 
\begin{figure} 
  \begin{minipage}[t]{0.475\textwidth}
    \centering
    \includegraphics[width=\textwidth]{Images/mode_dropdown.png}
    \caption{The DD for the selection of the grouping-mode.}
    \label{modeDropdown}
  \end{minipage} \hfill
  \begin{minipage}[t]{0.475\textwidth}
    \centering
    \includegraphics[width=\textwidth]{Images/filter.png}
    \caption{The input field with submit button for filtering.}
    \label{filterNodesElement}
  \end{minipage}   
\end{figure}   

 An \textit{input field} with a \textit{submit button} was placed underneath the DD for the purpose of filtering nodes according to a search phrase as can be seen in Figure\ref{filterNodesElement}.

The last element shown is an indicator of the current level - how many times the user zoomed into a community. The indicator has a \textit{zoom-out button} placed next to it.

The moment the data are retrieved from the BE the \textit{loader} gets replaced for a graph. The graph-nodes represent either the communities the pages are partitioned into or the pages themselves.  If the graph contains any isolated nodes (isolates) a \textit{mock-community} is displayed containing all isolates. This community cannot be zoomed into. A community node is depicted as a \textit{pie chart}. The individual sectors of the \textit{pie chart} represent the categories of the pages belonging into the community. This is portrayed in Figure\ref{communityNodeRepresentation}. It was difficult to distinct between a small community of only one category and page nodes. A page node is therefore differentiated from a community node by depicting the node as a square shaped symbol. The colour of the square corresponds to the category of the page. An example of a page is illustrated in Figure\ref{pageNodeRepresentation}. It is possible for a level to depict communities and pages at once. The links between the nodes visualize the links between pages or communities. 

\begin{figure} 
  \begin{minipage}[t]{0.475\textwidth}
    \centering
    \includegraphics[width=0.7\textwidth]{Images/community_node.png}
    \caption{The~representation of a~community node. The~individual sectors of the~pie-chart illustrate the~category-composition in the community.}
    \label{communityNodeRepresentation}
  \end{minipage} \hfill
  \begin{minipage}[t]{0.475\textwidth}
    \centering
    \includegraphics[width=0.7\textwidth]{Images/page_node.png}
    \caption{The~representation of a~page node. The~colour of the~square symbolizes the~category of the~page.}
    \label{pageNodeRepresentation}
  \end{minipage} 
\end{figure} 
After single-clicking a node additional information is exampled in the \textit{sidebar}.The details vary depending on whether the node is representing a community or a single page. 
The details of an individual page are as follows: 
\begin {description}
	\item [Url] which also serves as a unique identificator of the page. 
	\item [Category] of the page. Each page belongs to exactly one category.
	\item[Links] to other pages displayed as a list of url addresses. There are up to ten links visible in the sidebar. The remaining links, if any, are downloadable in a text file. 
Figure\ref{sidebarPageDetails} contains a sidebar view with page details.
\end{description}

The details of a community consist of grouped information of its members and include the following:
\begin {description}
	\item [Category composition] which is aggregated from the categories of all the pages of the community. Each category is represented by its name and the percentage of its relevance in the community.
	\item [Page url] addresses (urls) of members belonging into the community. There are up to ten urls visible in the sidebar. The remaining urls, if any, are downloadable in a text file. 
	\item[Urls count]represents the number of all the pages belonging into the community. 
\end{description}
Figure\ref{sidebarPageDetails} contains a sidebar view with community details.

\begin{figure}
  \begin{minipage}[t]{0.475\textwidth}
    \centering
    \includegraphics[width=\textwidth]{Images/community_sidebar.png}
    \caption{The sidebar with details of the selected community.}
   \label{sidebarCommunityDetails}
  \end{minipage} \hfill
  \begin{minipage}[t]{0.475\textwidth}
    \centering
    \includegraphics[width=\textwidth]{Images/page_sidebar.png}
    \caption{The sidebar with details of the selected page.}
    \label{sidebarPageDetails}
  \end{minipage} 
\end{figure}

In case of the need of further information a link, also present in the \textit{sidebar}, can be clicked. If clicked, a \textit{pop-up window} with detail-options is displayed. Details may include the \textit{title}, \textit{category}, \textit{links} and \textit{page-content}, depending on the user's selection. \textit{Urls} of the pages are present by default. The window can be seen in Figure\ref{detailsOptionsPopup}. After selecting the needed options and clicking the \textit{download button}, a \textit{text file} with the desired information is downloaded. The page or community-members with their details are depicted in JSON format. This functionality was required for the scenario of the user needing to download all available data about members of a specific community.
\begin{figure}[ht!]
  \centering
  \includegraphics[width=0.9\textwidth]{Images/options_popup.png}
  \caption{The pop-up window with detail-options for selection. These options dictate which details will be included in the downloaded text file.}
  \label{detailsOptionsPopup}
\end{figure} 

A graph node representing a community can be double clicked. After doing so, a new graph is shown. The data of this graph consists of the members of the clicked community. We call this process \textit{zooming}. In case the parent community contains too many sub-communities, it is displayed as a single community-node. The zooming-in or -out of communities adjusts the \textit{zoom-level}. This level is represented by a number and is visible below the \textit{node-filter}. If the current zoom level is more than zero, i.e. at least one community-node was double clicked, a button for zooming out appears next to the \textit{level indicator}. It is shown in Figure \ref{levelIndicatorWithButton}.
\begin{figure}[ht!]
  \centering
  \includegraphics{Images/levelIndicatorWithButton.png}
  \caption{The level indicator with the zoom-out button. After the zoom-out button is clicked, the user is shown the communities of the previous level.}
  \label{levelIndicatorWithButton}
\end{figure} 

If further zooming is not possible, the user reached the maximum level. Each community may have a different maximum level, depending on the number of its pages and its structure.

\subsection{Technology overview}
\textit{JavaScript} \cite{javaScript} (JS) is the most favoured programming language used for creating web applications \cite{jsGithut}. It is an interpreted language supported by all modern browsers. It is open-source and as such disposes of a considerable community with convenient documentation. JS is not strongly typed and the code might therefore be complicated to read or navigate. For this reason the FE was written in \textit{TypeScript} (TS) \cite{typeScript} which is a superset of JS with the advantage of being typed. There is a significant number of tools and libraries for implementing user interfaces (UI) in a clean and timely manner created for both JS and TS. One of such tools is the framework \textit{React.js} \cite{react} (React). React is one of the most favoured JS frameworks \cite{reactPopularity}. The advantages of using React are readable code and improved performance by managing the re-rendering of page elements.

To achieve a satisfying UX the app needed to be interactive and obtain new or modified data frequently. Repeated requests to the BE would mean longer wait time for the user. However, a proper mechanism for data-storing would present a convenient solution to this issue and make the requests unnecessary. A JS library which handles the app state and works well with TS and React is called \textit{Redux.js} \cite{redux} (Redux). Redux is a single store approach. This ensures easy hydration\footnote{The process of an object being provided with information}. Redux also provides a custom set of TS typings and provides the developers with easy to use debugging tools. Another advantage of this library is the documentation.

The visualization of the web graph alone was realized using the \textit{react-d3-graph} library \cite{reactD3Graph} (RD3). RD3 is 
an implementation of the library D3.js \cite{d3} made more convenient for the use with React.js. 

\subsection{Implementation}
The FE project consists of three folders and several configuration files. The folder \texttt{node\_modules} contains imported libraries including \textit{React.js}, \textit{Redux} or \textit{d3}. The next folder named \texttt{public} encloses a \textit{.ico} file\footnote {A picture with the dimensions 16x16 pixels used by the browser to represent the web page or application. It is usually displayed in the tab in which the application is opened.} and a html file which is the default entry point when the application is started. The last folder \texttt{src} contains the source code itself. 

As previously mentioned, the FE is written in TS which has the advantage of readability and easy navigation. There are, however, also disadvantages. One of them is the need of a TS file with the types (typing) for every used library. Typings for popular libraries are often downloadable as modules. If a library has no ready-to-download typings own ones need to be written. In our case the typings for the library \textit{react-d3-graph} were custom made. They can be found in the folder \texttt{@types/react-d3-graph}. The file \texttt{commont.d.ts} holds types used heavily across the application, e.g. \textit{Action}. Types in this file are available without importing them to all files in the project.  

Objects passed between functions also need to be typed. Those models are stored in the folder \texttt{models}. Each file contains one server-model and one client-model. The conversion between these models is conducted in specific helper functions. The advantage of this approach is the independence of client-models from the BE models.

The visual aspect is implemented using Less \cite{less} which is a language extending CSS with improvements such as the possibility of using variables. The Less classes are divided into files depending on the element they are meant to modify. These files were placed into the folder \texttt{styles}. 

The remaining folders each represent a different part of the UI. The structure of their sub-folders is similar. Therefore it is sufficient to describe them as a whole. Folders named \texttt{utils} contain files with helper functions such as converters between server and client models. \texttt{Constants} contains folders with string constants or simple functions which return a string depending on the input. The rest of the folders represent some part of the Redux framework. 

The most basic files which only include string constants are situated in the folders named \texttt{actionTypes}.  These are utilised as action types in actions. An action is a simple objects containing a type and an optional payload. Actions themselves are returned by action creators (AC). AC are functions returning an action and can be found in folders called \texttt{actions}. They can be as simple as those present in the file \texttt{nodesActionCreators.ts}. But they can be more complicated such as the AC \texttt{fetchNodes.ts} and dispatch multiple simple ACs. The purpose of an AC is to be injected into \hyperlink{reducers}{reducers}, i.e. dispatch them.

\texttt{fetchNodes} and the folder it is placed in share the same name. For easier testing purposes the main logic of this AC is put into a function which receives the simple ACs as dependencies. When this AC is called it first dispatches a simple AC to indicate the fetching has begun. After that an identificator (id) is initialized. This id is later used to create an error object in case of failure. Next, the fetching itself begins. The fetching in \texttt{fetchNodes} is realized with the library \textit{isomorphic-fetch}. The fetch function of this library expects the first argument to be the url address of the resource. The second argument is an object describing further details of the request and is optional. Such an object may contain the request method, headers or the payload. If the request does not result in error the response status is checked. After the fetching is complete a success AC with the acquired response is dispatched. If an error is caught during the fetching a failure AC is dispatched. The payload of this AC is an error-object with the id and error message if any.

The dispatching of actions enables the changing of the state via reducers situated in the \texttt{reducers} folders. The state is a single immutabe\footnote{The object cannot be adjusted directly. Instead, a new modified object is returned and the original one stays unmodified.} object and is used in the whole application. A \hypertarget{reducers}{reducer} is a pure function\footnote{The return value of a pure function is only dependent on its input values. A pure function has no side effects.} receiving the current state and the dispatched action as its arguments. It then returns the newly computed state. A reducer builds a new state only if the type of the given action is recognized. If not, the previous state is returned unmodified. The state object received or returned by the reducer does not need to be the entire state (app-state). A reducer may be responsible for just a part of the app-state. However, the root reducer is responsible for the whole app-state.

The folders \texttt{components} contain files with React components. They define the skeleton of the UI with the specified behaviour. The folders \texttt{containers} hold files with React containers. A container is a file with access to the app-state. It is responsible for passing data to components. 
