This chapter will describe the design and implementation of the application which is composed of a representational state transfer (REST) application program interface (API) and a front-end (FE) web application. 

\section{API}
Since the scraped pages of the dark-web were stored via ElasticSearch it was necessary to create a back-end application (BE) in order to perform various operations on the data-set before sending it to the FE. We decided to create a Python BE since the application had to be able to run on a UNIX system. 
\subsection{Technology overview}
Python is a widely used interpreted programming language known for its  readability and portability \cite{aboutPython}. It is open-source and is considered to have an extensive documentation and community available. Another huge advantage is its popularity in the science community. Because of this there is a great amount of useful libraries for research purposes such as NetworkX\footnote{A library used for creating and working with graphs.} \cite{networkX} or cylouvain \footnote{A library with a fast implementation of LA.} \cite{cylouvain}.  
As we wanted to follow the REST architecture we decided to make use of the Django framework \cite{meetDjango}. It is responsible for tasks such as running the server or managing web requests. Another advantage of Django is its Django REST framework (DRF). DRF offers a convenient way for creating restful endpoints and responses. \cite{djangoRest}. Both frameworks are open-source again with extremely helpful documentation and community. 


Because it takes approximately 60 seconds to retrieve about 90,000 pages from the database and circa 13 seconds to divide such a response into communities, caching had to be introduced. For that purpose Redis \cite{redis} is used. It is an open-source solution which we use as a key-value store. It supports  basic data structures as values, e.g. strings, numbers or sets but not custom objects. Since the API uses custom objects for both communities and pages, an object serializer had to be leveraged along with Redis. We decided not to write our own but to utilise the python pickle module \footnote{A module used for converting python objects to streams of bytes and vice versa.} \cite{pickle}. 

\section{Front-end}
For users to be able to see the data acquired from the BE in a reasonable way a FE application was created. 
\subsection{Technology overview}
The probably most favoured programming language used for creating web applications \cite{jsGithut} is called JavaScript (JS) \cite{javaScript}. It is an interpreted language supported by all modern browsers. It is open-source and as such disposes of a big community with splendid documentation. Because it is not strongly typed it the code might be complicated to read or navigate. For this reason the FE was written in TypeScript (TS) \cite{typeScript} which is a superset of JS with the advantage of being typed. JS or TS comes a significant amount of tools used for implementing user interfaces (UI) in a clean and timely manner. One of the most favoured frameworks is React.js \cite{react} which when used correctly results in readable code and improves performance by managing the re-rendering of page elements.


To be able to supply the FE with needed data for user interactions a store had to be implemented. React.js and TS work extremely well with a state container called Redux \cite{redux} and because of that we decided to leverage it. It is centralized with only one store, easy to debug and again has a convenient documentation. 

The doubtlessly most important part of this FE is the visualization of the graph built on the data sent from the BE. This feature is built using the react-d3-graph library \cite{reactD3Graph} which is 
an implementation of the library d3.js \cite{d3} made more convenient for the use with React.js. 

\subsection{User interface}
Users can view how the DW is partitioned into communities and how the communities are linked with one another. They can obtain details about each community, such as
\begin {description}
	\item [Category composition] which is aggregated from the categories of all the pages of the community. Each category is represented by its name and the percentage of its relevance in the community.
	\item [Page url] belonging into the community.There are up to ten urls visible in the sidebar. The rest of the them, if any, is downloadable in a text file. 
	\item[Urls count]represents the number of all the pages belonging into the community. 
\end{description}
Users may zoom into each community to view its sub-communities to the very last level where individual nodes are displayed. If zoomed into at least one community a label with a button appears in the sidebar indicating the current level. 

